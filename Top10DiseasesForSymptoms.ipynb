{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d46b310d-68ac-4d6c-9d82-37687ee40bdb",
   "metadata": {},
   "source": [
    "## This module finds top 10 possible diseases based on each ML model for given symptoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "045d3cb7-b0d4-4c0a-bb08-5d3e666f0d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter & ignore warnings for clear output visualization\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dd7c340-58cd-450f-bc15-5219c06f661f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary packages\n",
    "\n",
    "import os\n",
    "import math\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from statistics import mean\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d01daba-ed23-4906-afdc-1e440433ec74",
   "metadata": {},
   "source": [
    "### Functions to find co-occuring symptoms with some threshold & process ML results to get list of diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a3bf671-cfc2-445e-9e1d-9ad251e90714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate top 10 diseases from the ML results\n",
    "# Pass mean_score as another argument if you need probabilities too\n",
    "def ProcessResultAndGenerateDiseases(top10_list, mean_score, cooccuring_symptoms, user_symptoms_len):\n",
    "    \n",
    "    global df_independent, all_symptoms, all_diseases\n",
    "    top10_dict = {}\n",
    "\n",
    "    # Checks for each disease, the matched symptoms & generates probability of having that disease\n",
    "    for (idx, disease_id) in enumerate(top10_list):\n",
    "        matched_symptoms = set()\n",
    "        top10 = df_independent.loc[df_independent['Disease_Name'] == all_diseases[disease_id]].values.tolist()\n",
    "        \n",
    "        # Obtains the disease name which is at the top of the dataframe\n",
    "        disease = top10[0].pop(0)\n",
    "\n",
    "        # Each row contains 0s & 1s indicating whether a disease is associated with a particular symptom or not\n",
    "        for (idx, value) in enumerate(top10[0]):\n",
    "            if value != 0:\n",
    "                matched_symptoms.add(all_symptoms[idx])\n",
    "                \n",
    "        #print(\"\\n\", matched_symptoms)\n",
    "        probability = (len(matched_symptoms.intersection(set(cooccuring_symptoms))) + 1) / (user_symptoms_len + 1)\n",
    "        top10_dict[disease] = round(probability * mean_score * 100, 2)\n",
    "    \n",
    "    top10_sorted_dict = dict(sorted(top10_dict.items(), key=lambda kv: kv[1], reverse=True))\n",
    "    return top10_sorted_dict  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f865d3df-4991-4efb-8a62-1b34b9eb1481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display the results from the dictionary\n",
    "def PrintDictionary(top10_sorted_dict):\n",
    "    for (key, value) in top10_sorted_dict.items():\n",
    "        print(key, \"\\t\", value, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4170c8d-c847-4cf7-b989-ffb63e5e5363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate subsets\n",
    "def GetPossibleSubsets(user_symptoms):\n",
    "    \n",
    "    global all_symptoms\n",
    "    processed_symptoms = []\n",
    "    user_symptoms_len = len(user_symptoms)\n",
    "    minSubsetLength = math.floor(user_symptoms_len * 0.8)\n",
    "    \n",
    "    # Form possible subsets with minSubsetLength\n",
    "    for combination in range(minSubsetLength, user_symptoms_len + 1):\n",
    "        for subset in combinations(user_symptoms, combination):\n",
    "            temp_processed_symptoms = [0 for x in range(0, len(all_symptoms))]\n",
    "            for symptom in subset:\n",
    "                temp_processed_symptoms[all_symptoms.index(symptom)] = 1\n",
    "            processed_symptoms.append(temp_processed_symptoms)\n",
    "    \n",
    "    return processed_symptoms\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cd28e4b-5c65-4bb4-80ca-7e279c6b8b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get predictions for possible subsets from given symptoms\n",
    "def GetTop10BySubsets(model, mean_score, user_symptoms, processed_symptoms):\n",
    "    \n",
    "    model_dict_res, res_dict = {}, {}\n",
    "    user_symptoms_len = len(user_symptoms)\n",
    "    subsets = 0\n",
    "    \n",
    "    for proc_sym in processed_symptoms:\n",
    "        subsets += 1\n",
    "        model_result = model.predict_proba([proc_sym])\n",
    "        model_top10 = model_result[0].argsort()[-10:][::-1]\n",
    "        model_dict = ProcessResultAndGenerateDiseases(model_top10, mean_score, user_symptoms, user_symptoms_len)\n",
    "\n",
    "        for (key, value) in model_dict.items():\n",
    "            if key not in model_dict_res.keys():\n",
    "                model_dict_res[key] = [value, 1]\n",
    "            else:\n",
    "                model_dict_res[key] = [model_dict_res[key][0] + value, model_dict_res[key][1] + 1]\n",
    "        #print(model_dict_res, \"\\n\")\n",
    "    \n",
    "    print(\"Total no. of subsets considered: \", subsets)\n",
    "    for (key, value) in model_dict_res.items():\n",
    "        res_dict[key] = round(value[0] / value[1], 2)\n",
    "        \n",
    "    res_dict = dict(sorted(res_dict.items(), key=lambda item: item[1], reverse=True)[:10])\n",
    "    return res_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c34b0524-0503-49b6-8617-9b264e1072d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find co-occuring symptoms with all the symptoms user chosen\n",
    "# We use a threshold to check for a 90% match with the given symptoms\n",
    "\n",
    "def FindCooccuringSymptomsWithThreshold(user_symptoms):\n",
    "    \n",
    "    global df_independent, all_symptoms\n",
    "    threshold = math.floor(len(user_symptoms) * 0.90)\n",
    "\n",
    "    # Get all unique possible diseases with the given symptoms\n",
    "    unique_diseases = set()\n",
    "    for symptom in user_symptoms:\n",
    "        possible_diseases_for_symptom = list(df_independent[df_independent[symptom] == 1]['Disease_Name'])\n",
    "        for disease in possible_diseases_for_symptom:\n",
    "            unique_diseases.add(disease)\n",
    "        \n",
    "    # Get all unique diseases & sort them\n",
    "    unique_diseases = sorted(list(unique_diseases))\n",
    "    \n",
    "    #print(unique_diseases)\n",
    "\n",
    "    # Obtain co-occuring symptoms with 90% threshold\n",
    "    # cooccuring_symptoms must have all given symptoms by default\n",
    "    cooccuring_symptoms = set(user_symptoms)   \n",
    "    for disease in unique_diseases:\n",
    "        \n",
    "        # First, obtain all symptoms associated with each disease in unique diseases obtained\n",
    "        symptoms_of_disease = df_independent.loc[df_independent['Disease_Name'] == disease].values.tolist().pop(0)\n",
    "\n",
    "        # Maintain a temporary set of symptoms of the disease & add them only when they meet threshold requirements\n",
    "        temp_symptoms = set()\n",
    "        count, add_symptoms = 0, False\n",
    "        for idx in range(len(symptoms_of_disease)):\n",
    "            \n",
    "            # Symptoms of a disease will have 1 in their respective symptom columns\n",
    "            if symptoms_of_disease[idx] == 1:\n",
    "                temp_symptoms.add(all_symptoms[idx-1])\n",
    "                count = count + 1\n",
    "\n",
    "                # Our threshold is set to 90% of original symptoms\n",
    "                if count > threshold:\n",
    "                    add_symptoms = True\n",
    "\n",
    "        # Adds temporary symptoms to cooccuring symptoms only if they meet threshold requirements\n",
    "        if add_symptoms == True:\n",
    "            for symp in temp_symptoms:\n",
    "                cooccuring_symptoms.add(symp)\n",
    "\n",
    "    cooccuring_symptoms = sorted(list(cooccuring_symptoms))\n",
    "    return cooccuring_symptoms\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47463f7c-1a4e-4ce9-9577-75c19377915a",
   "metadata": {},
   "source": [
    "### Prepares data to be compatible with the dataset to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0cf07e3-e84f-4211-8936-18a0b2337536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets for all possible combinations & for individual disease's respective symptoms\n",
    "current_directory = os.getcwd()\n",
    "data_path = current_directory + \"/Datasets-CSV\"\n",
    "\n",
    "df_combination = pd.read_csv(data_path + \"/Disease_Symptom_Dataset_For_All_Symptom_Subsets.csv\") \n",
    "df_independent = pd.read_csv(data_path + \"/Disease_Symptom_Dataset_For_Respective_Symptoms.csv\") \n",
    "\n",
    "X_combination = df_combination.iloc[:, 1:]\n",
    "Y_combination = df_combination.iloc[:, 0:1]\n",
    "\n",
    "X_independent = df_independent.iloc[:, 1:]\n",
    "Y_independent = df_independent.iloc[:, 0:1]\n",
    "\n",
    "# List of all possible symptoms\n",
    "all_symptoms = list(X_independent.columns)\n",
    "all_diseases = list(set(Y_independent['Disease_Name']))\n",
    "all_diseases.sort()\n",
    "\n",
    "# We obtain top 10 possible diseases\n",
    "no_of_diseases = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad4139cc-7bea-4dba-ae62-a1735adb32d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will be obtained from the front-end\n",
    "#user_symptoms = ['back', 'shortness breath', 'stomach pain', 'chest pain', 'cold sweat', 'feeling faint', 'feeling tired']\n",
    "user_symptoms = ['headache', 'light sensitivity', 'sound', 'nausea', 'irritation', 'muscle joint pain']\n",
    "user_symptoms_len = len(set(user_symptoms))\n",
    "\n",
    "# Get possible subsets with minimum 80% count\n",
    "processed_symptoms = GetPossibleSubsets(user_symptoms)\n",
    "\n",
    "# Obtains all possible cooccuring symptoms including given symptoms\n",
    "cooccuring_symptoms = FindCooccuringSymptomsWithThreshold(user_symptoms)\n",
    "processed_symptoms2 = [0 for x in range(0, len(all_symptoms))]\n",
    "for symptom in cooccuring_symptoms:\n",
    "    processed_symptoms2[all_symptoms.index(symptom)] = 1\n",
    "\n",
    "processed_symptoms.append(processed_symptoms2)\n",
    "#print(processed_symptoms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f3f881-0355-4668-9777-e53d239f516b",
   "metadata": {},
   "source": [
    "### Uses 4 Machine Learning models to obtain possible predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f2f9227-5e05-474e-a7a1-f985bef644c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sav file path\n",
    "sav_path = current_directory + \"/Model-Weights/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b0830eb-0b32-4ed5-b6dd-da6c88a2b6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing with Logistic Regression...\n",
      "Total no. of subsets considered:  23\n",
      "Done\n",
      "\n",
      "Migraine \t 66.83 %\n",
      "Zika virus disease \t 40.1 %\n",
      "Dengue \t 40.1 %\n",
      "Glaucoma \t 26.73 %\n",
      "Lactose intolerance \t 26.73 %\n",
      "Kidney stone disease \t 26.73 %\n",
      "Iritis \t 26.73 %\n",
      "Keratoconus \t 26.73 %\n",
      "Hepatitis a \t 26.73 %\n",
      "Tetanus \t 26.73 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing with Logistic Regression...\")\n",
    "lr_cls = joblib.load(sav_path + \"log_reg.sav\")\n",
    "lr_mean_score = joblib.load(sav_path + \"log_reg_cv.sav\")\n",
    "lr_dict = GetTop10BySubsets(lr_cls, lr_mean_score, user_symptoms, processed_symptoms)\n",
    "print(\"Done\\n\")\n",
    "PrintDictionary(lr_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76bf665f-c1b8-49e8-9e33-06480bce635d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing with Random Forest Classifier...\n",
      "Total no. of subsets considered:  23\n",
      "Done\n",
      "\n",
      "Migraine \t 65.73 %\n",
      "Zika virus disease \t 39.44 %\n",
      "Dengue \t 39.44 %\n",
      "Glaucoma \t 26.29 %\n",
      "Exposure keratopathy \t 26.29 %\n",
      "Factitious keratoconjunctivitis \t 26.29 %\n",
      "Japanese encephalitis \t 26.29 %\n",
      "Listeriosis \t 26.29 %\n",
      "Iritis \t 26.29 %\n",
      "Astigmatism \t 26.29 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing with Random Forest Classifier...\")\n",
    "rf_cls = joblib.load(sav_path + \"rand_forest.sav\")\n",
    "rf_mean_score = joblib.load(sav_path + \"rand_forest_cv.sav\")\n",
    "rf_dict = GetTop10BySubsets(rf_cls, rf_mean_score, user_symptoms, processed_symptoms)\n",
    "print(\"Done\\n\")\n",
    "PrintDictionary(rf_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f37b9ad5-ef53-45f9-a502-d1462b3a18fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing with KNN Classifier...\n",
      "Total no. of subsets considered:  23\n",
      "Done\n",
      "\n",
      "Migraine \t 65.78 %\n",
      "Zika virus disease \t 39.47 %\n",
      "Dengue \t 39.47 %\n",
      "Glaucoma \t 26.31 %\n",
      "Exposure keratopathy \t 26.31 %\n",
      "Factitious keratoconjunctivitis \t 26.31 %\n",
      "Rocky mountain spotted fever \t 26.31 %\n",
      "Mumps \t 26.31 %\n",
      "Lyme disease \t 26.31 %\n",
      "Leptospirosis \t 26.31 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing with KNN Classifier...\")\n",
    "knn_cls = joblib.load(sav_path + \"knn.sav\")\n",
    "knn_mean_score = joblib.load(sav_path + \"knn_cv.sav\")\n",
    "knn_dict = GetTop10BySubsets(knn_cls, knn_mean_score, user_symptoms, processed_symptoms)\n",
    "print(\"Done\\n\")\n",
    "PrintDictionary(knn_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40619055-aceb-41aa-b447-8d365d1c3efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing with Multinomial Naive Bayes...\n",
      "Total no. of subsets considered:  23\n",
      "Done\n",
      "\n",
      "Migraine \t 64.52 %\n",
      "Zika virus disease \t 38.71 %\n",
      "Dengue \t 38.71 %\n",
      "Hepatitis a \t 25.81 %\n",
      "Mucormycosis \t 25.81 %\n",
      "Tetanus \t 25.81 %\n",
      "Flu \t 25.81 %\n",
      "Crimean congo haemorrhagic fever (cchf) \t 25.81 %\n",
      "Ebola \t 25.81 %\n",
      "Chickenpox \t 25.81 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing with Multinomial Naive Bayes...\")\n",
    "mnb_cls = joblib.load(sav_path + \"mnb.sav\")\n",
    "mnb_mean_score = joblib.load(sav_path + \"mnb_cv.sav\")\n",
    "mnb_dict = GetTop10BySubsets(mnb_cls, mnb_mean_score, user_symptoms, processed_symptoms)\n",
    "print(\"Done\\n\")\n",
    "PrintDictionary(mnb_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe747ce8-65c8-48fc-9e89-ea17c9d62734",
   "metadata": {},
   "source": [
    "### Process the obtained results to be suitable for UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b10c6bf6-1c00-445b-af68-526e74b1a52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use joint probabilities for the final dictionary & probabilities\n",
    "final_dict = {}\n",
    "\n",
    "# For Logistic Regression\n",
    "for (key, val) in lr_dict.items():\n",
    "    if key not in final_dict:\n",
    "        final_dict[key] = [lr_dict[key], 1]\n",
    "    else:\n",
    "        prob, count = final_dict[key]\n",
    "        final_dict[key] = [lr_dict[key] + prob, 1 + count]\n",
    "        \n",
    "# For Random Forest\n",
    "for (key, val) in rf_dict.items():\n",
    "    if key not in final_dict:\n",
    "        final_dict[key] = [rf_dict[key], 1]\n",
    "    else:\n",
    "        prob, count = final_dict[key]\n",
    "        final_dict[key] = [rf_dict[key] + prob, 1 + count]\n",
    "\n",
    "# For KNN Classifier\n",
    "for (key, val) in knn_dict.items():\n",
    "    if key not in final_dict:\n",
    "        final_dict[key] = [knn_dict[key], 1]\n",
    "    else:\n",
    "        prob, count = final_dict[key]\n",
    "        final_dict[key] = [knn_dict[key] + prob, 1 + count]\n",
    "        \n",
    "# For Multinomial Naive Bayes\n",
    "for (key, val) in mnb_dict.items():\n",
    "    if key not in final_dict:\n",
    "        final_dict[key] = [mnb_dict[key], 1]\n",
    "    else:\n",
    "        prob, count = final_dict[key]\n",
    "        final_dict[key] = [mnb_dict[key] + prob, 1 + count]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d565b62e-7323-422c-bba8-7fc6712bfdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%  65.72 \t75%  49 \t50%  32.86 \t25%  16.43 \n",
      "\n",
      "Migraine :\t count4\n",
      "Zika virus disease :\t count3\n",
      "Dengue :\t count3\n",
      "Glaucoma :\t count2\n",
      "Iritis :\t count1\n",
      "Exposure keratopathy :\t count1\n",
      "Factitious keratoconjunctivitis :\t count1\n",
      "Hepatitis a :\t count1\n",
      "Tetanus :\t count1\n",
      "Lactose intolerance :\t count1\n"
     ]
    }
   ],
   "source": [
    "# Obtain probability over max.count possible\n",
    "processed_dict = {}\n",
    "max_prob = 0\n",
    "for (key, val) in final_dict.items():\n",
    "    processed_dict[key] = round(final_dict[key][0] / 4, 2)\n",
    "    if processed_dict[key] > max_prob:\n",
    "        max_prob = processed_dict[key]\n",
    "    #print(key, \"...\", processed_dict[key], \"...\", final_dict[key][1])\n",
    "\n",
    "# Obtain likeliness range\n",
    "prob_100 = round(max_prob, 2)\n",
    "prob_50 = round(prob_100 / 2, 2)\n",
    "prob_25 = round(prob_50 / 2, 2)\n",
    "prob_75 = round(prob_50 + prob_25)\n",
    "\n",
    "# Visualize the probability ranges\n",
    "print(\"100% \", prob_100, \"\\t75% \", prob_75, \"\\t50% \", prob_50, \"\\t25% \", prob_25, \"\\n\")\n",
    "\n",
    "# Sort dictionary by probabilities & leave off the less possible ones\n",
    "final_dict = dict(sorted(processed_dict.items(), key=lambda item: item[1], reverse=True)[:10])\n",
    "#PrintDictionary(final_dict)\n",
    "\n",
    "# Set count values by range\n",
    "for key in final_dict.keys():\n",
    "    prob, count = final_dict[key], 0\n",
    "    if prob <= prob_100 and prob > prob_75:\n",
    "        count = 4\n",
    "    elif prob <= prob_75 and prob > prob_50:\n",
    "        count = 3\n",
    "    elif prob <= prob_50 and prob > prob_25:\n",
    "        count = 2\n",
    "    else:\n",
    "        count = 1\n",
    "    final_dict[key] = \"count\" + str(count)\n",
    "    print(key, \":\\t\", final_dict[key])\n",
    "\n",
    "# Pass the final_dict to the UI --> We call this in views.py\n",
    "#return render(request, \"index.html\", {\"final_dict\": final_dict, 'disable': True, 'show': False, 'back': True})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
